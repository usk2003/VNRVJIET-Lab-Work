{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM3EgwpWkxNwTKeVMM1sxIz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **ML Lab - Single Layer & Multi Layer Perceptron**\n","Urlana Suresh Kumar - 22071A6662"],"metadata":{"id":"N4AYHSytl48J"}},{"cell_type":"markdown","source":["In this notebook, we explore the implementation of **Multi-Layer Perceptrons (MLPs)** for image classification on the MNIST dataset. The MNIST dataset contains images of handwritten digits (0 to 9), which are commonly used for benchmarking machine learning models. We will demonstrate the following two types of MLPs:\n","\n","1. **Single Layer Perceptron**: A basic neural network with one hidden layer that learns to classify the MNIST digits based on pixel intensity values.\n","   \n","2. **Multi-Layer Perceptron**: A more advanced neural network with multiple hidden layers that is expected to perform better by capturing more complex patterns in the data.\n","\n","For both models, we will use **TensorFlow** and **Keras** to build and train the neural networks. We will normalize the dataset, reshape the image data, and evaluate the models' performance using test accuracy and loss."],"metadata":{"id":"6z0vir0xmRCs"}},{"cell_type":"markdown","source":["#**Single Layer Perceptron for MNIST Classification**"],"metadata":{"id":"y6GA-2qGmZ7P"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kRxtTh2flHOC","executionInfo":{"status":"ok","timestamp":1731334896963,"user_tz":-330,"elapsed":31553,"user":{"displayName":"Suresh Kumar Urlana (Study)","userId":"02832329327666247405"}},"outputId":"8d1b2256-2909-460f-d20e-249c42f1a42e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Training data shape: (60000, 28, 28) (60000,)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8118 - loss: 0.7260\n","Epoch 2/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9126 - loss: 0.3092\n","Epoch 3/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9205 - loss: 0.2838\n","Epoch 4/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9240 - loss: 0.2705\n","Epoch 5/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9260 - loss: 0.2656\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9150 - loss: 0.3030\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.2666223347187042, 0.9254999756813049]"]},"metadata":{},"execution_count":2}],"source":["# Import necessary libraries\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","# %matplotlib inline\n","\n","from tensorflow.keras.datasets import mnist\n","\n","# Load the dataset\n","d = mnist.load_data()\n","d\n","\n","(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n","print(\"Training data shape:\", x_train.shape, y_train.shape)\n","\n","# Normalizing the dataset\n","x_train = x_train/255\n","x_test = x_test/255\n","\n","# Flatten the dataset in order to compute for model building\n","x_train_flatten = x_train.reshape(len(x_train), 28*28)\n","x_test_flatten = x_test.reshape(len(x_test), 28*28)\n","\n","# Define the model\n","model = keras.Sequential([\n","    keras.layers.Dense(10, input_shape=(784,), activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(\n","    optimizer='adam',\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","# Train the model\n","model.fit(x_train_flatten, y_train, epochs=5)\n","\n","# Evaluate the model\n","model.evaluate(x_test_flatten, y_test)"]},{"cell_type":"markdown","source":["# **Multi-Layer Perceptron for MNIST Classification**\n","\n"],"metadata":{"id":"vu9UnlZ8m1KH"}},{"cell_type":"code","source":["# Import necessary libraries\n","import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Activation\n","import matplotlib.pyplot as plt\n","\n","# Load dataset\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","# Normalize the dataset\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","\n","# Normalize image pixel values by dividing by 255\n","gray_scale = 255\n","x_train /= gray_scale\n","x_test /= gray_scale\n","\n","print(\"Feature matrix:\", x_train.shape)\n","print(\"Target matrix:\", x_test.shape)\n","print(\"Feature matrix:\", y_train.shape)\n","print(\"Target matrix:\", y_test.shape)\n","\n","# Define the model architecture\n","model = Sequential([\n","    # Reshape 28x28 data to a 1D vector of 784 features\n","    Flatten(input_shape=(28, 28)),\n","\n","    # Dense layer 1\n","    Dense(256, activation='sigmoid'),\n","\n","    # Dense layer 2\n","    Dense(128, activation='sigmoid'),\n","\n","    # Output layer\n","    Dense(10, activation='sigmoid'),\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(x_train, y_train, epochs=10,\n","          batch_size=2000,\n","          validation_split=0.2)\n","\n","# Evaluate the model\n","results = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss, test accuracy:', results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5y8riLnemwgh","executionInfo":{"status":"ok","timestamp":1731335033159,"user_tz":-330,"elapsed":22178,"user":{"displayName":"Suresh Kumar Urlana (Study)","userId":"02832329327666247405"}},"outputId":"f132c3f4-34b7-41f9-fe41-2da1bd009907"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature matrix: (60000, 28, 28)\n","Target matrix: (10000, 28, 28)\n","Feature matrix: (60000,)\n","Target matrix: (10000,)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - accuracy: 0.2561 - loss: 2.2831 - val_accuracy: 0.6393 - val_loss: 1.7867\n","Epoch 2/10\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.6828 - loss: 1.6231 - val_accuracy: 0.7830 - val_loss: 1.1175\n","Epoch 3/10\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.7812 - loss: 1.0321 - val_accuracy: 0.8496 - val_loss: 0.7339\n","Epoch 4/10\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - accuracy: 0.8466 - loss: 0.7033 - val_accuracy: 0.8815 - val_loss: 0.5398\n","Epoch 5/10\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.8759 - loss: 0.5384 - val_accuracy: 0.8965 - val_loss: 0.4338\n","Epoch 6/10\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.8919 - loss: 0.4414 - val_accuracy: 0.9052 - val_loss: 0.3722\n","Epoch 7/10\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - accuracy: 0.9018 - loss: 0.3813 - val_accuracy: 0.9119 - val_loss: 0.3341\n","Epoch 8/10\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9090 - loss: 0.3439 - val_accuracy: 0.9183 - val_loss: 0.3065\n","Epoch 9/10\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9155 - loss: 0.3126 - val_accuracy: 0.9226 - val_loss: 0.2866\n","Epoch 10/10\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.9172 - loss: 0.2983 - val_accuracy: 0.9254 - val_loss: 0.2710\n","Test loss, test accuracy: [0.2776733636856079, 0.9221000075340271]\n"]}]},{"cell_type":"markdown","source":["# Conclusion\n","\n","In this notebook, we successfully implemented both **Single Layer Perceptron** and **Multi-Layer Perceptron** models to classify handwritten digits from the MNIST dataset.\n","\n","- The **Single Layer Perceptron** achieved decent accuracy with minimal complexity, while the **Multi-Layer Perceptron** showed improved performance with more hidden layers and better ability to capture intricate patterns in the data.\n","  \n","Both models utilized the **sigmoid activation function** and **Adam optimizer** for training, with **sparse categorical cross-entropy** as the loss function. The models were evaluated based on their test accuracy and loss, with the multi-layer perceptron generally performing better.\n","\n","This demonstrates the power of multi-layer neural networks in improving classification tasks, and serves as a foundational understanding of how to build and train MLP models in **TensorFlow** and **Keras**."],"metadata":{"id":"q8EuAk4Zm_ne"}}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO1JzmNlXIhNcEb3DEwh5PO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **ML Lab - XGBoost Algorithm**\n","Urlana Suresh Kumar - 22071A6662"],"metadata":{"id":"e6uhwmoB_m1W"}},{"cell_type":"markdown","source":["In this notebook, we explore the implementation of XGBoost for multi-class classification using the Iris dataset. The Iris dataset is a classic example in machine learning, consisting of features of three types of Iris flowers. The objective is to predict the flower type based on the given features.\n"],"metadata":{"id":"CDzyt1i6_pf3"}},{"cell_type":"markdown","source":["## Step 1: Import Libraries and Load Dataset\n","We start by importing the necessary libraries, loading the Iris dataset, and preparing it for training.\n"],"metadata":{"id":"TfyLL0yK_u2w"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"uS-aVUhA-fkQ","executionInfo":{"status":"ok","timestamp":1732046213790,"user_tz":-330,"elapsed":7246,"user":{"displayName":"Suresh Kumar Urlana (Study)","userId":"02832329327666247405"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score\n","import xgboost as xgb\n","\n","# Load Dataset\n","iris = load_iris()\n","iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n","iris_df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n","\n","# Features (X) and Target (y)\n","X = iris_df.iloc[:, :-1]  # Features (all columns except target)\n","y = iris_df['species']    # Target (species column)\n","\n","# Encode the target labels to numeric values\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)"]},{"cell_type":"markdown","source":["## Step 2: Split Data and Prepare for XGBoost\n","We split the dataset into training and testing sets and prepare it for training with XGBoost's DMatrix."],"metadata":{"id":"9hfo51i5__gd"}},{"cell_type":"code","source":["# Split into Training and Testing Data\n","X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n","\n","# Create DMatrix for XGBoost\n","dtrain = xgb.DMatrix(X_train, label=y_train)\n","dtest = xgb.DMatrix(X_test, label=y_test)\n"],"metadata":{"id":"y4bx2yWZABs_","executionInfo":{"status":"ok","timestamp":1732046238130,"user_tz":-330,"elapsed":475,"user":{"displayName":"Suresh Kumar Urlana (Study)","userId":"02832329327666247405"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Step 3: Define Parameters and Train the XGBoost Model\n","We define the model's parameters and train it on the training data."],"metadata":{"id":"pSOrArSnAE8J"}},{"cell_type":"code","source":["# Define XGBoost Parameters\n","params = {\n","    'objective': 'multi:softmax',  # For multi-class classification\n","    'num_class': 3,               # 3 classes in the Iris dataset\n","    'eval_metric': 'mlogloss',    # Multiclass log-loss\n","    'max_depth': 3,               # Maximum depth of a tree\n","    'eta': 0.1,                   # Learning rate\n","    'subsample': 0.8,             # Subsampling ratio\n","    'colsample_bytree': 0.8,      # Subsample ratio of columns for each tree\n","}\n","\n","# Train the XGBoost model\n","num_round = 100\n","bst = xgb.train(params, dtrain, num_round)\n"],"metadata":{"id":"_RvwGeQEAHX_","executionInfo":{"status":"ok","timestamp":1732046269140,"user_tz":-330,"elapsed":495,"user":{"displayName":"Suresh Kumar Urlana (Study)","userId":"02832329327666247405"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Step 4: Make Predictions and Evaluate the Model\n","We make predictions on the test data, convert numerical predictions to class labels, and evaluate the model's accuracy."],"metadata":{"id":"g_tCqnD8AOTq"}},{"cell_type":"code","source":["# Make Predictions\n","y_pred = bst.predict(dtest)\n","\n","# Convert numerical predictions back to class labels\n","y_pred_labels = label_encoder.inverse_transform(y_pred.astype(int))\n","\n","# Check actual labels from the test set\n","y_test_labels = label_encoder.inverse_transform(y_test)\n","\n","# Calculate Accuracy\n","accuracy = accuracy_score(y_test_labels, y_pred_labels)\n","\n","# Display Results\n","print(\"Predictions (original class labels):\", y_pred_labels[:10])  # First 10 predictions\n","print(\"Actual labels:\", y_test_labels[:10])  # First 10 actual labels\n","print(f'Accuracy: {accuracy * 100:.2f}%')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wmy-YuyRAK8h","executionInfo":{"status":"ok","timestamp":1732046296257,"user_tz":-330,"elapsed":470,"user":{"displayName":"Suresh Kumar Urlana (Study)","userId":"02832329327666247405"}},"outputId":"6efaa145-04cd-42c5-85c5-e9869372d2b6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions (original class labels): ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor' 'setosa'\n"," 'versicolor' 'virginica' 'versicolor' 'versicolor']\n","Actual labels: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor' 'setosa'\n"," 'versicolor' 'virginica' 'versicolor' 'versicolor']\n","Accuracy: 100.00%\n"]}]},{"cell_type":"markdown","source":["# Conclusion\n","In this notebook, we successfully implemented and evaluated the XGBoost algorithm for multi-class classification on the Iris dataset. The model achieved 100% accuracy, demonstrating its effectiveness in this scenario. XGBoost's powerful tree-based approach makes it a robust choice for classification tasks."],"metadata":{"id":"OASfdTkEAVod"}}]}